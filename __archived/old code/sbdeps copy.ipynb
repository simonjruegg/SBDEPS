{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64217962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Search Trial 1 ===\n",
      "Config: optimizer=sgd, lr=0.0005, filters=[64, 128], dense=256, dropouts=(0.4, 0.2), pooling=avg, batch_size=32\n",
      "Epoch 1/2\n",
      "\u001b[1m 5/29\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 5s/step - accuracy: 0.4803 - loss: 0.7331"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import tensorflow\n",
    "from itertools import product\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPooling2D, Input\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "\n",
    "\n",
    "MODE = \"random_search\"  # Set to \"fixed\" for fixed pipeline or \"random_search\" for random search\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "LABELS = ['no', 'yes']  # Class labels\n",
    "IMG_SIZE = 250          # Image resize\n",
    "np.random.seed(42)      # Numpy Seed\n",
    "EPOCHS = 2              # Number of epochs\n",
    "BATCH_SIZE = 32         # Batch size\n",
    "KFOLD_SEED = 42         # KFold Seed\n",
    "N_SPLITS = 2            # Number of KFold splits\n",
    "\n",
    "\n",
    "\n",
    "def get_data(data_dir):\n",
    "    X, y = [], []\n",
    "    data_dir = Path(data_dir)\n",
    "    for label in LABELS:\n",
    "        path = data_dir / label\n",
    "        class_num = LABELS.index(label)\n",
    "        for img_file in path.iterdir():\n",
    "            try:\n",
    "                img_arr = cv2.imread(str(img_file))[..., ::-1]\n",
    "                resized_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))\n",
    "                X.append(resized_arr)\n",
    "                y.append(class_num)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {img_file.name}: {e}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def augment_images(x, y):\n",
    "    x_aug, y_aug = [], []\n",
    "\n",
    "    for img, label in zip(x, y):\n",
    "        x_aug.append(img)  # 1. original\n",
    "        y_aug.append(label)\n",
    "\n",
    "        # 2. random rotation\n",
    "        k1 = np.random.choice([1, 2, 3])\n",
    "        rotated = np.rot90(img, k1)\n",
    "        x_aug.append(rotated)\n",
    "        y_aug.append(label)\n",
    "\n",
    "        # 3. different rotation (next clockwise) + flip\n",
    "        k2 = 1 if k1 == 3 else k1 + 1\n",
    "        rotated2 = np.rot90(img, k2)\n",
    "        flipped = np.fliplr(rotated2)\n",
    "        x_aug.append(flipped)\n",
    "        y_aug.append(label)\n",
    "\n",
    "    return np.array(x_aug), np.array(y_aug)\n",
    "\n",
    "\n",
    "def create_model_with_tuning(conv_filters, dense_units, dropout_conv, dropout_dense, pooling_type, optimizer_name, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "\n",
    "    for filters in conv_filters:\n",
    "        model.add(Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPool2D())\n",
    "\n",
    "    model.add(Dropout(dropout_conv))\n",
    "\n",
    "    if pooling_type == \"avg\":\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "    elif pooling_type == \"max\":\n",
    "        model.add(GlobalMaxPooling2D())\n",
    "    else:\n",
    "        raise ValueError(\"Invalid pooling_type. Use 'avg' or 'max'.\")\n",
    "\n",
    "    model.add(Dense(dense_units, activation=\"relu\"))\n",
    "    model.add(Dropout(dropout_dense))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())  # or use model.add(GlobalMaxPooling2D())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, custom_batch_size=BATCH_SIZE, custom_epochs=EPOCHS, custom_early_stop_p=5, custom_lr_p=3):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=custom_early_stop_p, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=custom_lr_p, min_lr=1e-5)\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=custom_batch_size,\n",
    "        epochs=custom_epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=[early_stop, lr_scheduler],\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return history\n",
    "\n",
    "def plot_metrics(history, fold):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(acc, label='Train Accuracy')\n",
    "    plt.plot(val_acc, label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'Fold {fold} - Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'Fold {fold} - Loss')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, x_val, y_val, fold):\n",
    "    predictions = (model.predict(x_val) > 0.5).astype(\"int32\").reshape(-1)\n",
    "    print(classification_report(y_val.astype(int), predictions, target_names=['no', 'yes']))\n",
    "\n",
    "    cm = confusion_matrix(y_val.astype(int), predictions)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=LABELS, yticklabels=LABELS)\n",
    "    plt.title(f'Fold {fold} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "def run_random_search_pipeline():\n",
    "    # Define search space\n",
    "    param_grid = {\n",
    "        \"optimizer_name\": [\"adam\", \"rmsprop\", \"sgd\"],\n",
    "        \"learning_rate\": [1e-4, 5e-4, 1e-3],\n",
    "        \"conv_filters\": [[32, 64], [64, 128], [32, 64, 128]],\n",
    "        \"dense_units\": [64, 128, 256],\n",
    "        \"dropout_conv\": [0.3, 0.4, 0.5],\n",
    "        \"dropout_dense\": [0.2, 0.3, 0.4],\n",
    "        \"pooling_type\": [\"avg\", \"max\"],\n",
    "        \"batch_size\": [8, 16, 32],\n",
    "    }\n",
    "\n",
    "    # Random sample N combinations\n",
    "    N_SEARCHES = 10\n",
    "    param_combinations = list(product(\n",
    "        param_grid[\"optimizer_name\"],\n",
    "        param_grid[\"learning_rate\"],\n",
    "        param_grid[\"conv_filters\"],\n",
    "        param_grid[\"dense_units\"],\n",
    "        param_grid[\"dropout_conv\"],\n",
    "        param_grid[\"dropout_dense\"],\n",
    "        param_grid[\"pooling_type\"],\n",
    "        param_grid[\"batch_size\"]\n",
    "    ))\n",
    "    random.shuffle(param_combinations)\n",
    "    param_combinations = param_combinations[:N_SEARCHES]\n",
    "    x_data, y_data = get_data('data')\n",
    "    x_data = x_data / 255.0\n",
    "    y_data = y_data.astype('float32')\n",
    "    \n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=KFOLD_SEED)\n",
    "\n",
    "    for i, (optimizer_name, lr, conv_filters, dense_units, drop_conv, drop_dense, pool, batch_size) in enumerate(param_combinations, 1):\n",
    "        print(f\"\\n=== Random Search Trial {i} ===\")\n",
    "        print(f\"Config: optimizer={optimizer_name}, lr={lr}, filters={conv_filters}, dense={dense_units}, dropouts=({drop_conv}, {drop_dense}), pooling={pool}, batch_size={batch_size}\")\n",
    "\n",
    "        fold_accuracies = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(x_data), 1):\n",
    "            x_train, x_val = x_data[train_idx], x_data[val_idx]\n",
    "            y_train, y_val = y_data[train_idx], y_data[val_idx]\n",
    "\n",
    "            x_train, y_train = augment_images(x_train, y_train)\n",
    "\n",
    "            model = create_model_with_tuning(conv_filters, dense_units, drop_conv, drop_dense, pool, optimizer_name, lr)\n",
    "            history = train_model(model, x_train, y_train, x_val, y_val, batch_size, custom_epochs=EPOCHS, custom_early_stop_p=5, custom_lr_p=3)  # You may modify to pass batch_size\n",
    "            print(f\"Fold {fold} training complete.\")\n",
    "\n",
    "            loss, acc = model.evaluate(x_val, y_val, verbose=0)\n",
    "            print(f\"Fold {fold} val_accuracy: {acc:.4f}\")\n",
    "            fold_accuracies.append(acc)\n",
    "\n",
    "        avg_acc = np.mean(fold_accuracies)\n",
    "        print(f\"→ Avg Validation Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "def run_kfold_pipeline():\n",
    "    x_data, y_data = get_data('data')\n",
    "    x_data = x_data / 255.0\n",
    "    y_data = y_data.astype('float32')\n",
    "\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=KFOLD_SEED)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(x_data), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        print(f\"Train samples: {len(train_idx)}, Validation samples: {len(val_idx)}\")\n",
    "        x_train, x_val = x_data[train_idx], x_data[val_idx]\n",
    "        y_train, y_val = y_data[train_idx], y_data[val_idx]\n",
    "\n",
    "        x_train, y_train = augment_images(x_train, y_train)\n",
    "        print(f\"Augmented Train samples: {len(x_train)}, Validation samples: {len(x_val)}\")\n",
    "        model = create_model()\n",
    "        print(\"model created.\")\n",
    "        # get string for the file names\n",
    "        now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        checkpoint = ModelCheckpoint(f'{now}model_fold_{fold}.h5', monitor='val_accuracy', save_best_only=True)\n",
    "        history = train_model(model, x_train, y_train, x_val, y_val)\n",
    "        print(\"Training complete.\")\n",
    "        evaluate_model(model, x_val, y_val, fold)\n",
    "        plot_metrics(history, fold)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if MODE == \"random_search\":\n",
    "        run_random_search_pipeline()\n",
    "    elif MODE == \"fixed\":\n",
    "        run_kfold_pipeline()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Use 'random_search' or 'fixed'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbdeps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
