{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9faa4c",
   "metadata": {},
   "source": [
    "This code was used to optimise our cnn with optuna, resulting in 2 high performing models:\n",
    "\n",
    "1. cnn_optimised_adam.keras\n",
    "- 4 Conv2D Layers 32->64->64->128 with batch normalisation and maxpool between layers\n",
    "- dropout of around 0.39 after last Conv2D layer\n",
    "- Global Maxpool\n",
    "- Dense(64) with Dropout of aroung 0.29\n",
    "- optimizer = adam\n",
    "- learning rate of 0.000724\n",
    "\n",
    "2. cnn_optimised_sgd.keras\n",
    "- 3 Conv2D Layers 32->128->128 with batch normalisation and maxpool between layers\n",
    "- dropout of around 0.42 after last Conv2D layer\n",
    "- Global Maxpool\n",
    "- Dense(256) with Dropout of aroung 0.25\n",
    "- optimizer = sgd\n",
    "- learning rate of 0.000139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64217962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPooling2D, Input\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "from datetime import datetime\n",
    "\n",
    "# Parameters\n",
    "N_TRIALS = 25\n",
    "EPOCHS = 30\n",
    "KFOLD_SEED = 42\n",
    "N_SPLITS = 5\n",
    "#BATCH_SIZE = 16\n",
    "LABELS = ['no', 'yes']\n",
    "IMG_SIZE = 250\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def get_data(data_dir):\n",
    "    X, y = [], []\n",
    "    data_dir = Path(data_dir)\n",
    "    for label in LABELS:\n",
    "        path = data_dir / label\n",
    "        class_num = LABELS.index(label)\n",
    "        for img_file in path.iterdir():\n",
    "            try:\n",
    "                img_arr = cv2.imread(str(img_file))[..., ::-1]\n",
    "                resized_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))\n",
    "                X.append(resized_arr)\n",
    "                y.append(class_num)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {img_file.name}: {e}\")\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X / 255.0\n",
    "    y = y.astype('float32')\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def augment_images(x, y):\n",
    "    x_aug, y_aug = [], []\n",
    "    for img, label in zip(x, y):\n",
    "        # 0° rotation (original)\n",
    "        x_aug.append(img)\n",
    "        y_aug.append(label)\n",
    "        x_aug.append(np.fliplr(img))  # horizontal flip\n",
    "        y_aug.append(label)\n",
    "        x_aug.append(np.flipud(img))  # vertical flip\n",
    "        y_aug.append(label)\n",
    "\n",
    "        # 90° rotation + flips\n",
    "        rot_90 = np.rot90(img, 1)\n",
    "        x_aug.append(rot_90)\n",
    "        y_aug.append(label)\n",
    "        x_aug.append(np.fliplr(rot_90))  # horizontal flip of 90°\n",
    "        y_aug.append(label)\n",
    "        x_aug.append(np.flipud(rot_90))  # vertical flip of 90°\n",
    "        y_aug.append(label)\n",
    "\n",
    "        # 180° rotation (no flips)\n",
    "        rot_180 = np.rot90(img, 2)\n",
    "        x_aug.append(rot_180)\n",
    "        y_aug.append(label)\n",
    "\n",
    "        # 270° rotation (no flips)\n",
    "        rot_270 = np.rot90(img, 3)\n",
    "        x_aug.append(rot_270)\n",
    "        y_aug.append(label)\n",
    "\n",
    "    return np.array(x_aug), np.array(y_aug)\n",
    "\n",
    "\n",
    "\n",
    "def create_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "\n",
    "    # Convert conv_filters string to tuple of ints\n",
    "    conv_filters = tuple(map(int, params['conv_filters'].split(',')))\n",
    "\n",
    "    for filters in conv_filters:\n",
    "        model.add(Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPool2D())\n",
    "\n",
    "    model.add(Dropout(params['dropout_conv']))\n",
    "\n",
    "    if params['pooling_type'] == 'avg':\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "    else:\n",
    "        model.add(GlobalMaxPooling2D())\n",
    "\n",
    "    model.add(Dense(params['dense_units'], activation='relu'))\n",
    "    model.add(Dropout(params['dropout_dense']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = {\n",
    "        'adam': Adam,\n",
    "        'rmsprop': RMSprop,\n",
    "        'sgd': SGD\n",
    "    }[params['optimizer_name']](learning_rate=params['learning_rate'])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, params, trial_number=None, fold_number=None):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5)\n",
    "    # get date and time\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d\")\n",
    "    # Create checkpoint filepath with trial and fold info\n",
    "    if trial_number is not None and fold_number is not None:\n",
    "        checkpoint_filepath = f'Model_{date_time}_trial_{trial_number}.h5'\n",
    "    else:\n",
    "        checkpoint_filepath = f'{date_time}_model_checkpoint.h5'\n",
    "\n",
    "    checkpoint_cb = ModelCheckpoint(checkpoint_filepath, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=EPOCHS,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[early_stop, lr_scheduler, checkpoint_cb],\n",
    "                        verbose=1,\n",
    "                        shuffle=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial, x_data=None, y_data=None):\n",
    "    print(f\"\\n🔍 Starting trial {trial.number}...\")\n",
    "\n",
    "    x_train, _, y_train, _ = train_test_split(x_data, y_data, test_size=0.2, stratify=y_data, random_state=42)\n",
    "    print(f\"  Split data: {x_train.shape[0]} training samples\")\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=KFOLD_SEED)\n",
    "\n",
    "    params = {\n",
    "        'optimizer_name': trial.suggest_categorical('optimizer_name', ('adam', 'rmsprop', 'sgd')),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "        'conv_filters': trial.suggest_categorical('conv_filters', [\n",
    "                '32,64,128',\n",
    "                '32,128,128',\n",
    "                '64,128,256',\n",
    "                '32,64,64,128',\n",
    "                '32,64,128,128',\n",
    "                '64,64,128,256',\n",
    "            ]),\n",
    "        'dense_units': trial.suggest_categorical('dense_units', (64, 128, 256)),\n",
    "        'dropout_conv': trial.suggest_float('dropout_conv', 0.25, 0.5),\n",
    "        'dropout_dense': trial.suggest_float('dropout_dense', 0.25, 0.5),\n",
    "        'pooling_type': trial.suggest_categorical('pooling_type', ('avg', 'max')),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', (8, 16, 32)),\n",
    "    }\n",
    "\n",
    "    print(f\"  Hyperparameters: {params}\")\n",
    "    f1_scores = []\n",
    "    all_y_val = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(x_train), 1):\n",
    "        print(f\"  ➡️ Fold {fold_num}/{N_SPLITS}\")\n",
    "        x_tr, x_val = x_train[train_idx], x_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        x_tr, y_tr = augment_images(x_tr, y_tr)\n",
    "        print(f\"    Augmented training data: {x_tr.shape[0]} samples\")\n",
    "\n",
    "        model = create_model(params)\n",
    "        print(f\"    Model created. Starting training...\")\n",
    "        model = train_model(model, x_tr, y_tr, x_val, y_val, params, trial_number=trial.number, fold_number=fold_num)\n",
    "        print(f\"    Training completed. Evaluating model...\")\n",
    "\n",
    "        y_pred_prob = model.predict(x_val)\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "        fold_f1 = f1_score(y_val, y_pred)\n",
    "        print(f\"    Fold {fold_num} F1 score: {fold_f1:.4f}\")\n",
    "        f1_scores.append(fold_f1)\n",
    "\n",
    "        # Collect all validation labels and predictions\n",
    "        all_y_val.extend(y_val)\n",
    "        all_y_pred.extend(y_pred)\n",
    "\n",
    "    # After all folds, print confusion matrix and classification report\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_y_val, all_y_pred))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_y_val, all_y_pred, target_names=LABELS))\n",
    "\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    print(f\"  🏁 Trial {trial.number} average F1 score: {avg_f1:.4f}\")\n",
    "    return avg_f1\n",
    "\n",
    "\n",
    "\n",
    "def run_optuna_pipeline(x_data=None, y_data=None):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, x_data, y_data), n_trials=N_TRIALS)\n",
    "    print(\"\\n🎉 Optimization finished. Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  F1 Score: {trial.value:.4f}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    # After your study is done:\n",
    "    fig = vis.plot_parallel_coordinate(study, target_name=\"F1 Score\")\n",
    "    fig.show()\n",
    "\n",
    "    fig = vis.plot_param_importances(study)\n",
    "    fig.show()\n",
    "\n",
    "    fig = vis.plot_slice(study, target_name=\"F1 Score\")\n",
    "    fig.show()\n",
    "\n",
    "    fig = vis.plot_optimization_history(study)\n",
    "    fig.show()\n",
    "\n",
    "    fig = vis.plot_intermediate_values(study)\n",
    "    fig.show()\n",
    "\n",
    "    fig = vis.plot_edf(study)\n",
    "    fig.show()\n",
    "\n",
    "    fig = vis.plot_contour(study, target_name=\"F1 Score\")\n",
    "    fig.show()\n",
    "\n",
    "def main():\n",
    "    print(\"🔍 Starting data loading...\")\n",
    "    x_data, y_data = get_data('data')\n",
    "    print(f\"  Loaded data: {x_data.shape[0]} samples\")\n",
    "    run_optuna_pipeline(x_data=x_data, y_data=y_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbdeps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
